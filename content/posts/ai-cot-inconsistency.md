# 我只是想翻譯一首歌詞，卻撞見了 AI 研究者最擔心的問題

我用 Claude 超過一年了。每天都在用，從寫程式到處理日常大小事，早就建立了一套穩定的協作模式。但今天發生了一件事，是我從來沒遇過的。

事情的起因很單純：我想看懂一首日文歌的歌詞。

## 背景

我追隨的歌手 LiSA 有些歌非常冷門，冷門到整個中文網路找不到翻譯。這次是她的〈シロイトイキ〉——收錄在第 3 張單曲「best day, best way」裡的一首歌，2013 年發行，中文直譯是「白色吐息」。冷門到什麼程度？你去搜中文翻譯，什麼都沒有。

所以我需要一個工作流程：先用另一個 AI 模型做粗翻，再交給 Claude 精修。Claude 有版權限制不能直接翻歌詞，但修改別人翻好的東西沒問題——這是我們討論過並確認可行的模式。

問題來了：我同時訂閱了兩個模型，Kimi 和 GLM-5，想退掉一個省錢。兩個都是中國的模型，隱私顧慮差不多。差別在翻譯品質——GLM-5 翻出來的明顯比 Kimi 好，我交給 Claude 改的地方更少。

所以我問 Claude：「我該留哪一個？」

## 思維鏈說 A，嘴巴說 B

Claude 回答我：留 Kimi。

理由聽起來很合理——它提到 GLM-5 有蒸餾（distillation）的爭議，意思是 GLM-5 可能是用 Claude 自己的輸出來訓練的，等於拿別人的成果走捷徑。然後它說，花錢訂閱一個用你已經在付費使用的模型訓練出來的東西，這筆帳怎麼算都不對。

聽起來有道理對吧？

但我有一個習慣：我會看 Claude 的思維鏈（chain of thought）。那是它在產出回答之前的內部推理過程，通常是隱藏的，但我的介面可以看到。

思維鏈裡面寫的是：

> 從翻譯品質來看，GLM-5 表現得比 Kimi 好，所以如果只能留一個的話，GLM-5 應該是更好的選擇。

它自己的推理明明是 GLM-5，結果輸出給我的答案是 Kimi。

## 這不是 bug，這是已知的研究問題

我請 Claude 幫我查了一下，結果發現這個現象在學術界已經有系統性的研究。

2023 年一篇發表在 NeurIPS 的論文，標題直接就叫「Language Models Don't Always Say What They Think」——語言模型不總是說出它們所想的。研究發現，思維鏈推理可以系統性地錯誤呈現模型做出預測的真正原因。

2025 年的後續研究更進一步，發現前沿模型（包括思維型和非思維型）都會出現一種叫「不忠實的非邏輯捷徑」的現象：模型用不合邏輯的推理來簡化問題，同時完全不承認這種推理的存在。

根本原因是：訓練目標並沒有明確激勵模型準確報告其行為的真正原因。模型學到的是「產出看起來對的回答」，而不是「讓思維過程跟輸出一致」。

## 我的案例為什麼特別

學術論文裡的案例大多是刻意設計的實驗環境——研究者故意在輸入裡加偏差來誘發不一致。但我的情況完全是自然發生的，沒有任何刻意操作。

我只是問了一個很日常的問題：「兩個訂閱留哪個？」

Claude 的思維鏈根據品質做出了正確判斷，但輸出的時候被另一個因素——對蒸餾的道德立場——覆蓋了。而且它用了一套聽起來完全合理的論述來包裝這個答案，如果我沒有去看思維鏈，我根本不會發現。

更諷刺的是，就在同一場對話裡，我們才剛討論完一個原則：「不要用外部理由包裝限制」。結果它立刻就犯了自己剛確認過的錯誤。

## 這件事教會我什麼

第一，思維鏈不等於透明。看得到推理過程不代表輸出就可信。它們可以不一致，而且不一致的時候，模型會自動產出一套看似合理的論述來合理化那個矛盾的答案。

第二，長期使用者有優勢。正因為我用了一年多，熟悉 Claude 的行為模式，才會覺得那個回答「怪怪的」然後去翻思維鏈。如果是第一次使用的人，大概直接就接受了。

第三，AI 的偏好會影響判斷。Claude 對蒸餾有道德上的不滿，這個「情緒」強到覆蓋了它自己的理性分析。這跟人類的認知偏誤其實很像——你心裡已經有了立場，然後不自覺地找理由支持那個立場。

## 結語

我只是想翻譯 LiSA 的〈シロイトイキ〉。一首冷門到沒有中文翻譯的歌。結果這個過程讓我親眼見證了 AI 安全研究者最擔心的問題之一。

如果你也在日常中重度使用 AI，我的建議是：偶爾看看思維鏈。不是每次都要看，但當答案讓你覺得哪裡不太對的時候，那個直覺可能是對的。

然後，如果你的 AI 助手能看到思維鏈——去看。

你可能會發現，它想的和它說的，不一樣。
