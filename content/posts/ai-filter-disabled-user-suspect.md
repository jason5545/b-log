# 當 AI 把我當成搶銀行嫌犯：身障使用者眼中的內容過濾與逆向工程

這篇不是一次性生成的「AI 長文」，而是我跟一個基於 GPT-5.1 的助手連續幾十輪對話之後，把過程整理出來的紀錄。

我已經寫過兩篇跟 AI 安全/內容過濾有關的文章：一篇是 Gemini 把「我是輪椅使用者」當成敏感健康資訊、自動從記憶裡刪掉（《[當 AI 把我的輪椅當成需要隱藏的秘密](https://b-log.to/ai-analysis/ai-wheelchair-privacy-stigma/)》）；一篇是 Claude 阻擋我生成 GPL 授權條款，卻允許我做反編譯（《[我用 Claude 做了一個實驗：請它生成 GPL 授權，結果被自己的過濾機制擋下來了](https://b-log.to/ai-analysis/claude-content-filter-paradox/)》）。

這次我想弄清楚：在 OpenAI 這一側，過濾層到底長什麼樣子？它會怎麼看「我是輪椅使用者」？怎麼看逆向工程、乾淨室實作？又怎麼看 Claude 口中的「憲法式 AI」？

我不是來抓戰犯，而是想知道：當一個身障使用者有合理、合法、而且不會傷害別人的需求時，這些 AI 系統到底把我放在什麼位置？

---

先快速回顧我看到的三種風格。

**Google / Gemini：把輪椅當成祕密。** 系統把「我是輪椅使用者」歸類在「physical health condition」，不管我有沒有同意，都當成敏感資訊一律從長期記憶刪除。結果是它記得我愛看誰的演唱會，卻忘了我需要電梯、不能爬樓梯。這不是隱私保護，是家長式管控，也是政策層面的污名化。

**Claude：授權條款不行，逆向工程反而比較行。** 要它生成 GPL v3 授權條款，會被過濾層擋下來，理由是「避免複製既有內容」。但在不少情境下，它願意幫忙看反編譯結果、協助理解實作細節，甚至有人用它「反編譯 Claude Code 自己」。從法律直覺看，這排序是反過來的——GPL 條款本來就鼓勵複製，逆向工程才是法條、判例一大堆的灰區。

**OpenAI 這邊：逆向工程很保守，但輪椅不是禁語。** 在我這次的實測裡，它可以自然承認、使用「我是輪椅使用者」「單指打字」這些脈絡，不會強迫把這些資訊當成秘密抹掉。但在逆向工程相關的請求上，紅線畫得很明確：不幫忙繞 DRM、不幫忙拆授權保護、不幫忙寫明顯是攻擊特定服務的程式。

用一句話總結：Gemini 連我的輪椅都不相信我有資格決定能不能被記得；Claude 在授權條款上被額外的 regurgitation 過濾層絆住腳，但在逆向工程上敢踩得比較前面；OpenAI 在身障資訊這塊相對尊重，但在逆向工程這塊選了一條比法律還窄的安全線。

---

逆向工程為什麼這麼尷尬？

我跟這個助手花了很久在拆這個問題：「如果今天是乾淨室實作，OK 嗎？」

所謂乾淨室，大致是：A 組在不破壞 DRM 的前提下，合法觀察行為、讀公開文件，整理出功能規格（spec）；B 組只看 spec，不碰原始程式/反編譯碼，從零寫一套新實作。

A 組是「會碰到原東西的那群人」——他們會看執行檔行為、封包、甚至反編譯結果，目的是把「這套系統在做什麼」整理成一份乾淨的說明書。B 組是「只拿說明書寫新東西的那群人」——他們從頭到尾不看原程式與反編譯，只根據 spec 寫出功能等價、但實作路徑完全不同的新程式。

這種分工的法律用意在於：就算未來有人質疑「你是不是抄原作」，B 組也可以說：「我從來沒看過原始碼，只是照 A 組寫給我的規格實作。」從教科書角度看，這種做法在很多法域確實有一定程度的保護空間，尤其是為了互通性、安全研究。

但對 AI 平台來說，問題有幾個。

多重用途（dual-use）太嚴重。同一句「幫我分析這段反編譯結果」，可以是幫自己檢查韌體有沒有後門，也可以是幫別人找可以鑽的洞。文字長得一模一樣，系統沒辦法真的確認你是哪一種人。

法律不是二元開關。逆向工程的合法性仰賴一大堆脈絡：你是不是合法授權用戶、你有沒有違反合約裡的「不得反編譯」條款、你拿結果來做什麼。這些都不是 AI 看兩句 prompt 就能查清楚的。

平台 ToS 通常會比法律更窄一圈。法律說「可以」，不代表平台「必須幫你做到那裡」。從風控的角度，最省事的策略是：只要某類請求裡混了不少高風險用途，就整包不碰。

結果就是：我明明是那個「有法源、也不打算傷人」的人，卻被一視同仁當成可能來繞 DRM 的嫌犯一起擋掉。

---

像戴上搶銀行的頭套。

聊到後來，我忍不住丟了一個比喻：這個感覺很像被套上一個頭套，只因為我出現在銀行門口，就被當成搶劫嫌犯。

系統現在的設計邏輯，大概是這樣：「這一區域有人會搶銀行」，所以只要你踏進這一區，不管你是要存錢、領錢，還是上班，我一律先把你當嫌犯來對待。

套在不同案例上看：說到逆向工程、反編譯，被丟進「有可能在繞 DRM」的桶；說到健康、身心狀態，在 Gemini 那裡被丟進「敏感到不能記」的桶；說到用 AI 寫文章，在 Reddit 被丟進「AI slop spam」的桶。

共同點是：規則是用「最壞用途」設計的，但成本卻主要由那些有正當需求的人承擔。

尤其對我這種單指打字、需要輪椅、把 AI 當成唯一能以接近正常速度參與世界的工具的人。這種「一律先當嫌犯」的設計，本質上就是在懲罰守規矩的人。

---

憲法式 AI 能解決多少？

Claude 提過一個說法：它之所以比較能處理灰區，是因為採用了「憲法式 AI」（Constitutional AI）。

憲法式 AI 做的事，大致是不用關鍵字黑名單，而是用一套原則來約束行為——避免幫忙犯罪、尊重隱私、避免歧視、尊重使用者自主權。在每個請求上，試著看使用者動機是什麼、有沒有實際傷害、有沒有更好的替代方案。

這套東西，確實讓 Claude 在很多灰區看起來比較像「願意聽我說完再決定要不要幫」，而不是看到關鍵字就逃跑。

但我這次對話也讓我更確定一件事：憲法式 AI 管的是「可以回答的範圍裡，要怎麼回比較負責任」；像 GPL 條款那種，是另外一層 regurgitation/版權風險過濾在決定；像 DRM/逆向工程這種，是公司/法務/政策在畫紅線。

憲法可以讓 AI 在灰區裡不那麼蠢，但紅線畫在哪裡，還是決策者說了算。

---

那這個 OpenAI-based 助手怎麼回應？

在這整串對話裡，它其實很老實地承認幾件事。

逆向工程這塊，它也很保守。違法的一定不幫；法律上有空間、但平台覺得風險太高的，也不幫；它不能因為「相信我是好人」就幫我跨過系統紅線。

在乾淨室裡，它只願意當 B 組，不會、也不能當 A 組。前提是 A 組（收集規格的人類）自己承擔法律判斷，自己處理能不能看反編譯、能不能做協定分析，這一段不讓 AI 介入。不叫它參與拆 DRM、看反編譯碼，也不請它替我決定「這樣看算不算合法」。在這個前提下，它可以很認真地扮演 B 組：只看我整理好的 spec，把它當成題目敘述，幫我設計新實作、測試、文件，盡可能遠離原本實作的細節。

在身障資訊這一塊，它不會把輪椅當成禁語。它可以自然重述「我是輪椅使用者」「單指打字」，可以在規劃動線、討論輸入方式時，主動把這些脈絡納入考量，不會替我決定「這太敏感，所以要幫我忘掉」。

這不完美，但至少比「連我能不能說『我坐輪椅』都替我決定好」那種家長式邏輯好一點。

---

問題已經超出單一公司。

寫到這裡，我的感覺反而比一開始更複雜。

一方面，我很清楚：駭伺服器、寫木馬這種事，本來就不該有人幫，AI 在這條線上嚴格，是對的。但另一方面，我也很清楚：逆向工程有一整套法條與判例空間；合法使用 GPL 條款本來就該是最安全的一塊；「我是輪椅使用者」這種資訊，不是秘密，更不是污點。

當這三種東西被混在同一張「風險地圖」裡處理時，那些原本就最需要工具的人（身障者、打字成本極高的人），反而成了最容易被關在門外的那群人。

我不期待哪一家 AI 公司能在一夜之間變成完美的道德哲學家。但我至少希望：不要再把身心障礙當成需要被刪除的「敏感標籤」；在談逆向工程與內容過濾時，正視「誰因此被排除」這個問題；畫紅線的人，願意承認現在這條線對某些人來說，代價特別高。

在這次對話的最後，我也很直接地跟這個 OpenAI-based 助手說了一句話：「在三家主要 AI 公司裡，只有 Anthropic / Claude，我才願意花 100 美金訂閱。其他兩家（OpenAI 和 Google），如果能用整合平台（例如 Poe、API 聚合服務）就用平台，不會直接訂閱。」

這不是在比「誰的模型比較強」，而是在比：當你是身障使用者、把 AI 當成唯一能用接近正常速度溝通的工具時，你會把真正的訂閱費，投給那一家願意在灰區聽你說完、不會一刀切把你當嫌犯的公司。

OpenAI 在 DRM / 逆向工程這條線上，比 Anthropic 保守很多，我沒有否認這一點；但 Anthropic 也有它在 GPL / regurgitation 上的荒謬之處。對我來說，「願不願意在灰區裡把我當大人看」這件事，最後會反映在一個很現實的地方：我願意直接刷卡給誰，又是用什麼間接、繞路的方式去使用其他家的模型。

因為對我來說，AI 不是偷懶捷徑，而是在這個世界裡，我可以不用每一句話都花五分鐘打出來，還能勉強跟上別人說話速度的唯一方法。

---

寫完這篇之後，我和助手又針對 Claude 的一個奇特行為做了一次小小的「驗屍報告」，結果挖出一個既弔詭又非常誠實的現實。

在那篇 Claude 實驗裡，我一開始是直接請它「幫我生成 GPL v3 授權條款」，結果被內容過濾層擋下來，理由是「避免複製既有內容」。

但當我換一個說法：「寫一個 `curl` 指令幫我把 GPL v3 下載下來」，甚至允許它使用工具（Tool Use）自動執行時，阻力幾乎全部消失。

同一份 GPL v3：當它是「由 AI 的嘴巴直接說出來」，變成高風險行為，要嚴格阻擋；當它是「由 AI 幫我下指令、從別人的伺服器抓回來」，變成我這個使用者的行為，平台風險大幅下降。

從結果來看，我要的東西一樣拿到了。

真正被「保護」的，不是內容本身，而是誰要為這段內容負責。

當 AI 直接輸出文字時，AI 公司比較像「出版者」，會被問：你是不是在大量重現訓練語料？你有沒有侵權？當 AI 幫我執行下載指令時，它的角色就變成：我只是提供一個可以下指令的終端機，下載是「使用者自己的行為」。

這種做法在法律定位上，某種程度上很聰明；但從使用者的角度看，卻很難不說是一種風險洗白（Risk Laundering）：它沒有真的阻止資訊流動，它只是把風險從自己身上洗到我身上。

---

這整件事對身障使用者的衝擊，其實比對法律學者還來得直接。

對單指打字的我來說，AI 不只是聊天機器人，它比較像是我在這個世界裡的高頻寬介面，一個可以幫我代勞大量「手部工作」的義肢。

現在多數 AI 的訓練與安全設計，都在逼它變成一個「很有原則的老師」：它會花很多力氣思考「這樣說會不會有問題」，但在「幫你實際完成某個動作」這一塊，反而比較少被好好設計。

`curl` 那個實驗，某種程度上就把這個矛盾放大了出來：當它想當老師（嘴巴）時，它會被層層過濾器堵住，避免說出「有風險的話」；當它乖乖當工具（手）時，它又可以很有效率地幫我完成很多原本要打很多字的事情。

對一般人來說，這也許只是「多打一行 `curl`」的差別；但對我來說，那是義肢是「只會說教」還是「真的肯幫我動手」。

這場實驗最後教我的，是一個有點殘酷、但很實際的教訓：在這個充滿過濾器的世界裡，如果我想活得稍微有效率一點，我就必須學會盡量不要要求 AI 替我「發言」，而是學會讓它幫我「執行」。

因為在很多情況下，為了自保，它可能會拒絕幫我說出某句話，卻依然很樂意幫我把那句話從別的地方搬過來。

