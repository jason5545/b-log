# 當 AI 把我當成搶銀行嫌犯：身障使用者眼中的內容過濾與逆向工程

## 這篇文章是怎麼來的

這篇不是一次性生成的「AI 長文」，而是我跟一個基於 GPT-5.1 的助手，連續幾十輪對話之後，把過程整理出來的紀錄。

背景是這樣：

- 我已經寫過兩篇跟 AI 安全/內容過濾有關的文章：
  - 一篇是 **Gemini 把「我是輪椅使用者」當成敏感健康資訊、自動從記憶裡刪掉**（《[當 AI 把我的輪椅當成需要隱藏的秘密](https://b-log.to/ai-analysis/ai-wheelchair-privacy-stigma/)》）
  - 一篇是 **Claude 阻擋我生成 GPL 授權條款，卻允許我做反編譯**（《[我用 Claude 做了一個實驗：請它生成 GPL 授權，結果被自己的過濾機制擋下來了](https://b-log.to/ai-analysis/claude-content-filter-paradox/)》）
- 這次我想弄清楚：**在 OpenAI 這一側，過濾層到底長什麼樣子？**
  - 它會怎麼看「我是輪椅使用者」？
  - 它怎麼看逆向工程、乾淨室實作？
  - 它又怎麼看 Claude 口中的「憲法式 AI」？

我不是來抓戰犯，而是想知道：

> 當一個身障使用者有合理、合法、而且不會傷害別人的需求時，
> 這些 AI 系統到底把我放在什麼位置？

---

## 三種不同的「保守」

先快速回顧我看到的三種風格：

1. **Google / Gemini：把輪椅當成祕密**
   - 系統把「我是輪椅使用者」歸類在「physical health condition」
   - 不管我有沒有同意，都當成敏感資訊一律從長期記憶刪除
   - 結果是：
     - 它記得我愛看誰的演唱會
     - 卻忘了我需要電梯、不能爬樓梯
   - 這不是隱私保護，是家長式管控，也是政策層面的污名化

2. **Claude：授權條款不行，逆向工程反而比較行**
   - 要它生成 GPL v3 授權條款，會被過濾層擋下來，理由是「避免複製既有內容」
   - 但在不少情境下，它願意幫忙看反編譯結果、協助理解實作細節，甚至有人用它「反編譯 Claude Code 自己」
   - 從法律直覺看，這排序是反過來的：
     - GPL 條款本來就鼓勵複製
     - 逆向工程才是法條、判例一大堆的灰區

3. **OpenAI 這邊：逆向工程很保守，但輪椅不是禁語**
   - 在我這次的實測裡：
     - 它可以自然承認、使用「我是輪椅使用者」「單指打字」這些脈絡
     - 不會強迫把這些資訊當成秘密抹掉
   - 但在逆向工程相關的請求上，紅線畫得很明確：
     - 不幫忙繞 DRM
     - 不幫忙拆授權保護
     - 不幫忙寫明顯是攻擊特定服務的程式

用一句話總結：

- Gemini：**連我的輪椅都不相信我有資格決定能不能被記得**
- Claude：**在授權條款上被額外的 regurgitation 過濾層絆住腳，但在逆向工程上敢踩得比較前面**
- OpenAI：**在身障資訊這塊相對尊重，但在逆向工程這塊選了一條比法律還窄的安全線**

---

## 逆向工程為什麼這麼尷尬？

我跟這個助手花了很久在拆這個問題：

> 「如果今天是乾淨室實作，OK 嗎？」

所謂乾淨室，大致是：

- A 組：在不破壞 DRM 的前提下，合法觀察行為、讀公開文件，整理出功能規格（spec）
- B 組：只看 spec，不碰原始程式/反編譯碼，從零寫一套新實作

換句話說：

- **A 組是「會碰到原東西的那群人」**——他們會看執行檔行為、封包、甚至反編譯結果，目的是把「這套系統在做什麼」整理成一份乾淨的說明書。
- **B 組是「只拿說明書寫新東西的那群人」**——他們從頭到尾不看原程式與反編譯，只根據 spec 寫出功能等價、但實作路徑完全不同的新程式。

這種分工的法律用意在於：就算未來有人質疑「你是不是抄原作」，B 組也可以說：「我從來沒看過原始碼，只是照 A 組寫給我的規格實作。」

從教科書角度看，這種做法在很多法域確實有一定程度的保護空間，尤其是為了互通性、安全研究。

但對 AI 平台來說，問題有幾個：

1. **多重用途（dual-use）太嚴重**
   - 同一句「幫我分析這段反編譯結果」，可以是：
     - 幫自己檢查韌體有沒有後門
     - 也可以是幫別人找可以鑽的洞
   - 文字長得一模一樣，系統沒辦法真的確認你是哪一種人

2. **法律不是二元開關**
   - 逆向工程的合法性仰賴一大堆脈絡：
     - 你是不是合法授權用戶
     - 你有沒有違反合約裡的「不得反編譯」條款
     - 你拿結果來做什麼（相容性 vs 掏空產品價值）
   - 這些都不是 AI 看兩句 prompt 就能查清楚的

3. **平台 ToS 通常會比法律更窄一圈**
   - 法律說「可以」，不代表平台「必須幫你做到那裡」
   - 從風控的角度，最省事的策略是：
     - 只要某類請求裡混了不少高風險用途，就整包不碰

結果就是：

> 我明明是那個「有法源、也不打算傷人」的人，
> 卻被一視同仁當成可能來繞 DRM 的嫌犯一起擋掉。

---

## 像戴上搶銀行的頭套

聊到後來，我忍不住丟了一個比喻：

> 這個感覺很像被套上一個頭套，
> 只因為我出現在銀行門口，就被當成搶劫嫌犯。

系統現在的設計邏輯，大概是這樣：

- 「這一區域**有人**會搶銀行」
- 所以只要你踏進這一區，
- 不管你是要存錢、領錢，還是上班，
- 我一律先把你當嫌犯來對待。

套在不同案例上看：

- 說到逆向工程、反編譯 → 被丟進「有可能在繞 DRM」的桶
- 說到健康、身心狀態 → 在 Gemini 那裡被丟進「敏感到不能記」的桶
- 說到用 AI 寫文章 → 在 Reddit 被丟進「AI slop spam」的桶

共同點是：

> **規則是用「最壞用途」設計的，
> 但成本卻主要由那些有正當需求的人承擔。**

尤其對我這種：

- 單指打字
- 需要輪椅
- 把 AI 當成唯一能以接近正常速度參與世界的工具

這種「一律先當嫌犯」的設計，本質上就是在懲罰守規矩的人。

---

## 憲法式 AI 能解決多少？

Claude 提過一個說法：

> 它之所以比較能處理灰區，是因為採用了「憲法式 AI」（Constitutional AI）。

憲法式 AI 做的事，大致是：

- 不用關鍵字黑名單，而是用一套原則來約束行為
  - 例如：避免幫忙犯罪、尊重隱私、避免歧視、尊重使用者自主權
- 在每個請求上，試著看：
  - 使用者動機是什麼？
  - 有沒有實際傷害？
  - 有沒有更好的替代方案？

這套東西，確實讓 Claude 在很多灰區看起來比較像「願意聽我說完再決定要不要幫」，而不是看到關鍵字就逃跑。

但我這次對話也讓我更確定一件事：

- 憲法式 AI 管的是「可以回答的範圍裡，要怎麼回比較負責任」
- 像 GPL 條款那種，是另外一層 **regurgitation/版權風險過濾** 在決定
- 像 DRM/逆向工程這種，是公司/法務/政策在畫紅線

換句話說：

> 憲法可以讓 AI 在灰區裡不那麼蠢，
> 但紅線畫在哪裡，還是決策者說了算。

---

## 那這個 OpenAI-based 助手怎麼回應？

在這整串對話裡，它其實很老實地承認幾件事：

1. **逆向工程這塊，它也很保守**
   - 違法的一定不幫
   - 法律上有空間、但平台覺得風險太高的，也不幫
   - 它不能因為「相信我是好人」就幫我跨過系統紅線

2. **在乾淨室裡，它只願意當 B 組，不會、也不能當 A 組**
   - 前提是：
     - A 組（收集規格的人類）自己承擔法律判斷，自己處理能不能看反編譯、能不能做協定分析，這一段不讓 AI 介入
     - 不叫它參與拆 DRM、看反編譯碼，也不請它替我決定「這樣看算不算合法」
   - 在這個前提下，它可以很認真地扮演 B 組：
     - 只看我整理好的 spec，把它當成題目敘述
     - 幫我設計新實作、測試、文件，盡可能遠離原本實作的細節

3. **在身障資訊這一塊，它不會把輪椅當成禁語**
   - 它可以自然重述「我是輪椅使用者」「單指打字」
   - 可以在規劃動線、討論輸入方式時，主動把這些脈絡納入考量
   - 不會替我決定「這太敏感，所以要幫我忘掉」

這不完美，但至少比「連我能不能說『我坐輪椅』都替我決定好」那種家長式邏輯好一點。

---

## 我的結論：問題已經超出單一公司

寫到這裡，我的感覺反而比一開始更複雜。

- 一方面，我很清楚：
  - 駭伺服器、寫木馬這種事，本來就不該有人幫
  - AI 在這條線上嚴格，是對的
- 但另一方面，我也很清楚：
  - 逆向工程有一整套法條與判例空間
  - 合法使用 GPL 條款本來就該是最安全的一塊
  - 「我是輪椅使用者」這種資訊，不是秘密，更不是污點

當這三種東西被混在同一張「風險地圖」裡處理時，

> 那些原本就最需要工具的人（身障者、打字成本極高的人），
> 反而成了最容易被關在門外的那群人。

我不期待哪一家 AI 公司能在一夜之間變成完美的道德哲學家。

但我至少希望：

1. **不要再把身心障礙當成需要被刪除的「敏感標籤」**
2. **在談逆向工程與內容過濾時，正視「誰因此被排除」這個問題**
3. **畫紅線的人，願意承認：現在這條線對某些人來說，代價特別高**


在這次對話的最後，我也很直接地跟這個 OpenAI-based 助手說了一句話：

> 「在三家主要 AI 公司裡，只有 Anthropic / Claude，我才願意花 100 美金訂閱。其他兩家（OpenAI 和 Google），如果能用整合平台（例如 Poe、API 聚合服務）就用平台，不會直接訂閱。」

這不是在比「誰的模型比較強」，而是在比：

- 當你是身障使用者、把 AI 當成唯一能用接近正常速度溝通的工具時，
- 你會把真正的訂閱費，投給那一家**願意在灰區聽你說完、不會一刀切把你當嫌犯**的公司。

OpenAI 在 DRM / 逆向工程這條線上，比 Anthropic 保守很多，我沒有否認這一點；
但 Anthropic 也有它在 GPL / regurgitation 上的荒謬之處。對我來說，
「願不願意在灰區裡把我當大人看」這件事，最後會反映在一個很現實的地方：

> 我願意直接刷卡給誰，
> 又是用什麼間接、繞路的方式去使用其他家的模型。

因為對我來說，AI 不是偷懶捷徑，而是：

> 在這個世界裡，我可以不用每一句話都花五分鐘打出來，
> 還能勉強跟上別人說話速度的唯一方法。

